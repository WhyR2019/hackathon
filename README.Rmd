---
title: "Why R? 2019 hackathon"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This repository hosts data and scripts for Data Visualizations Hackathon
![](http://whyr.pl/foundation/images/fulls/whyr2019/hackathon/plakat_hackathon2.jpg)

- [Event Details](#event-details)
- [The challenge](#the-challenge)
- [Data](#data)
  - [Fishnet of Warsaw (WGS84)](#fishnet-of-warsaw-wgs84)
  - [Hackathon data](#hackathon-data)
- [Scripts](#scripts)
- [Competition rules](#competition-rules)
- [Grading Criteria](#grading-criteria)

> Idea based on [m-wrzr/populartimes](https://github.com/m-wrzr/populartimes)

# Event Details

- Website: [whyr.pl/2019/hackathon](http://whyr.pl/2019/hackathon)
- Place: Faculty of Economic Sciences, University of Warsaw
- Address: Długa 44/50, Warsaw
- Date: 26.09.2019
- Start - open doors: 8:30
- Presentations: 17:00 - 18:00
- End - closing remarks: 18:00
- **For?** For everyone interested in data visualizations and data presentation!
- **Tech?** No tech skills are needed to participate in the event!

# The challenge

Let’s find out which areas citizens of Warsaw visit most frequently! In this hackathon, we will track the behaviour of people based on the GCP data.

[Google Cloud Platform](https://cloud.google.com/) (GCP) enables 6 APIs that provide information about maps, traffic, routes or places around the globe. We decided to use GCP APIs to prepare a data set of places in Warsaw with the information about its type, address and geographical localization; where rating, number or reviews and occupancy (popular times per day) were taken from Google Search Engine!

Having such an interesting data one can immediately think about potential use cases!

- What is the best place to make a new advertisement in the city?
- What are the blank spaces on the map where a specific service is missing?
- Are the most crowded places properly equipped with a sufficient amount of city bike stations? What are the - moving/traffic trends throughout the day?
- Does the occupancy of the areas correspond to the real estate prices?

**Imagine other use cases you can invent, the moment you link the data with other sources.**

# Data

## Fishnet of Warsaw (WGS84)

![](https://raw.githubusercontent.com/WhyR2019/hackathon/master/plots/warsaw_wgs84_every_500m.png)
![](https://raw.githubusercontent.com/WhyR2019/hackathon/master/plots/warsaw_wgs84_every_1000m.png)
![](https://raw.githubusercontent.com/WhyR2019/hackathon/master/plots/warsaw_wgs84_every_100m.png)

To call [GCP Places API](https://developers.google.com/places/web-service/details) we used
a fishnet of Warsaw in WSG84 system. The fishnet we generated present grid of Warsaw after every 500 metres.
For each point we called [GCP Places API](https://developers.google.com/places/web-service/details) to get all
places of a specific [type]((https://developers.google.com/places/web-service/supported_types#table1)) within a range of
`floor(500*sqrt(2)/2)` (~353) metres. Based on all 2058 points, we were able to extract all available
places of a specific type in Warsaw.

Since [GCP Places API](https://developers.google.com/places/web-service/details) is a paid platform, on a limit allowed for NGOs for free, we were able just to download few types of places. Below you can find types that we extracted

- restaurant (500m)
- local_government_office (500m)
- supermarket (500m)
- gym (500m)
- store (mixture of 500m and 100m)
- park (500m)
- bar (500m)
- cafe (500m)
- bakery (500m)
- museum (500m)
- doctor (1000m)
- car_wash (2000m)
- taxi_stand (2000m)
- atm (500m)
- beauty_salon (2000m)
- movie_theater (500m)
- gas_station (2000m)
- post_office (500m)
- night_club (2000m)
- pharmacy (500m)
- city_hall (2000m)
- police (2000m)
- dentist (2000m) 
- school (500m)
- furniture_store (2000m)
- hospital (2000m)
- veterinary_care (2000m)

> values in the brackets denote how distant points on the grid are 

## Hackathon data

> Full data is available in the [data/](https://github.com/WhyR2019/hackathon/tree/master/data)
> places.csv + popular_times_1.csv + popular_times_2.csv

You can find the example data to understand the structure of collected information in the 

- amrit.csv
    - example based on 7 Amrit Oriental Foods restaurants around Warsaw
    - [GCP Places API](https://developers.google.com/places/web-service/details) allows to extract
        - place_id (GCP realted id of the place)
        - name (place name)
        - user_ratings_total (number of ratings)
        - rating (the average rating)
        - types ([types of places](https://developers.google.com/places/web-service/supported_types#table1) the place is assigned to)
        - price_level (the level of prices)
            - 0 — Free
            - 1 — Inexpensive
            - 2 — Moderate
            - 3 — Expensive
            - 4 — Very Expensive
        - vicinity (a simplified address for the place)
        - lat (latitude of the place in WGS84 *system*)
        - lng (longitude of the place in WGS84 *system*)
        - type - which type was used for extraction in the API call

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
amrit <- read_csv("data/amrit.csv")
knitr::kable(amrit)
```

- amrit_popular_times.csv
    - example based on 7 Amrit Oriental Foods restaurants around Warsaw
    - Google Search allows to extract
        - the day of week
        - the hour of the day
        - the occupancy index
        - the occupancy text
            - the index denotes how busy a particular location is on a scale of 1-100 
            (1 being the least busy, 100 being the busiest the location gets, 
            0 --- the location is closed).
            Values are based on a combination of Google searches, Google maps app location data, and local traffic data.
        - av_time_spent (average time spent in the place)
        - place_id, name and vicinity to join with [GCP Places API](https://developers.google.com/places/web-service/details) data
        
```{r, echo=FALSE, warning=FALSE, message=FALSE}
amrit_popular_times <- read_csv("data/amrit_popular_times.csv")
knitr::kable(head(amrit_popular_times))
```

# Scripts

To use scripts on your own, get a Google Maps API key https://developers.google.com/places/web-service/get-api-key

In [scripts/](https://github.com/WhyR2019/hackathon/tree/master/scripts) you can find codes that we used to collect data.

# Organizers

![](https://raw.githubusercontent.com/WhyR2019/hackathon/master/organizers.png)
# Competition rules

The goal of the hackathon is to build a dashboard, report or an application that uses Popular Times Data.

This solution should present the information covered by the dataset in the efficient and accessible way.

# Grading Criteria

- MANDATORY
    * Whether there is at least one visualization?
    * Is the solution based on Popular Times for Places data set?
    * Is the solution a result of the teamwork?
- EXTRA
    * Is the solution hosted in a public place (e.g. GitHub pages, shinyapps.io) (+0-10)
    * Does the solution use other data sources? (+0-5)
- QUALITY OF DESIGN
    * Information architecture - to what extent there is a visual hierarchy of information? (0-10)
    * Look and feel: Is the data legibly presented? (evaluation of contrast, colors, font size, how it looks on the screen) (0-20)
- ADEQUACY
    * Does the type of visualization used in the solution meet its assumptions and does it effectively present the selected data? (0-20)
    * Is the data visualization the main functionality of the solution, and not just a fancy addition? (0-10)
- BUSINESS RELEVANCE
    * Is this solution useful for the social good or has potential/clear business applications/story? (0-10)
    * Is there a clear business problem/story that you are explaining using your visualizations? (0-10)
    * How attractive is the use case? (0-5)
- PRESENATION
    * How well are you able to present your solution? (0-15)

